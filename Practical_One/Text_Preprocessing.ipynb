{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f5f5ef",
   "metadata": {},
   "source": [
    "# Text Processing and Feature Extraction Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a comprehensive text processing pipeline using Python, spaCy, and scikit-learn. We'll explore:\n",
    "\n",
    "- **Data preprocessing** with pandas\n",
    "- **Natural Language Processing** using spaCy\n",
    "- **Text cleaning and normalization**\n",
    "- **Feature extraction** with Bag of Words and TF-IDF\n",
    "- **N-gram analysis** for enhanced text representation\n",
    "\n",
    "## Dataset\n",
    "We're working with a collection of sentences about lemons and lemonade to demonstrate various text processing techniques.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794d4b5",
   "metadata": {},
   "source": [
    "## 1. Data Setup and Initial Loading\n",
    "\n",
    "First, we'll import the necessary libraries and set up our sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa02e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"When life gives you lemons, make lemonade! ðŸ™‚\",\n",
    "    \"She bought 2 lemons for $1 at Maven Market.\",\n",
    "    \"A dozen lemons will make a gallon of lemonade. [AllRecipes]\",\n",
    "    \"lemon, lemon, lemons, lemon, lemon, lemons\",\n",
    "    \"He's running to the market to get a lemon â€” there's a great sale today.\",\n",
    "    \"Does Maven Market carry Eureka lemons or Meyer lemons?\",\n",
    "    \"An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]\",\n",
    "    \"iced tea is my favorite\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2756b1",
   "metadata": {},
   "source": [
    "### Creating Sample Dataset\n",
    "\n",
    "Our dataset contains 8 sentences with various text characteristics:\n",
    "- Mixed case text\n",
    "- Punctuation and special characters\n",
    "- Emojis and symbols\n",
    "- Numbers and prices\n",
    "- Citations and brackets\n",
    "- Repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62eba03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert list to DataFrame\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398b4ed",
   "metadata": {},
   "source": [
    "### Converting to DataFrame\n",
    "\n",
    "Let's convert our list of sentences into a pandas DataFrame for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a835ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set display options to show full content\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca94874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a copy for spaCy processing\n",
    "\n",
    "spacy_df = data_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "# Convert text to lowercase\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['sentence'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64f80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove specific citations\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace('[wikipedia]', '')\n",
    "\n",
    "\n",
    "\n",
    "# Advanced cleaning with regex\n",
    "\n",
    "combined = r'https?://\\S+|www\\.\\S+|<.*?>|\\S+@\\S+\\.\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s]'\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace(combined, ' ', regex=True)\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0eec2c",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing and Cleaning\n",
    "\n",
    "In this section, we'll clean our text data by:\n",
    "- Converting to lowercase\n",
    "- Removing URLs, email addresses, and social media handles\n",
    "- Removing special characters and punctuation\n",
    "- Normalizing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32f98f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./lib/python3.11/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./lib/python3.11/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./lib/python3.11/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./lib/python3.11/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./lib/python3.11/site-packages (from spacy) (0.16.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./lib/python3.11/site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./lib/python3.11/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./lib/python3.11/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.11/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in ./lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e468df",
   "metadata": {},
   "source": [
    "### Installing and Setting Up spaCy\n",
    "\n",
    "spaCy is an industrial-strength Natural Language Processing library that we'll use for:\n",
    "- Tokenization\n",
    "- Lemmatization  \n",
    "- Stop word removal\n",
    "- Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3136172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23337cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m109.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download and install English language model\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained pipeline\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "# Process a sample sentence\n",
    "\n",
    "phrase = spacy_df.clean_sentence[0] # \"when life gives you lemons make lemonade\"\n",
    "\n",
    "doc = nlp(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160408f8",
   "metadata": {},
   "source": [
    "### Loading spaCy Language Model\n",
    "\n",
    "Now we'll download the English language model and explore tokenization with a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59129a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[when, life, gives, you, lemons, make, lemonade]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract tokens as text strings\n",
    "\n",
    "[token.text for token in doc]\n",
    "\n",
    "# Output: ['when', 'life', 'gives', 'you', 'lemons', 'make', 'lemonade']\n",
    "\n",
    "\n",
    "\n",
    "# Extract tokens as spaCy objects (with linguistic attributes)\n",
    "\n",
    "[token for token in doc]\n",
    "\n",
    "# Output: [when, life, gives, you, lemons, make, lemonade]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bfcc0",
   "metadata": {},
   "source": [
    "### Exploring Tokenization\n",
    "\n",
    "Let's examine how spaCy breaks down our text into tokens and explore the difference between text strings and spaCy token objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddacdc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'life', 'give', 'you', 'lemon', 'make', 'lemonade']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract lemmatized forms\n",
    "\n",
    "[token.lemma_ for token in doc]\n",
    "\n",
    "# Output: ['when', 'life', 'give', 'you', 'lemon', 'make', 'lemonade']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991b9de",
   "metadata": {},
   "source": [
    "### Understanding Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or root form. For example:\n",
    "- \"gives\" â†’ \"give\"\n",
    "- \"lemons\" â†’ \"lemon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8137fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'life give lemon lemonade'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# View all English stop words in spaCy\n",
    "\n",
    "list(nlp.Defaults.stop_words)\n",
    "\n",
    "print(f\"Total stop words: {len(list(nlp.Defaults.stop_words))}\") # 326 stop words\n",
    "\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "\n",
    "[token for token in doc if  not token.is_stop]\n",
    "\n",
    "# Output: [life, gives, lemons, lemonade]\n",
    "\n",
    "\n",
    "\n",
    "# Combine lemmatization and stop word removal\n",
    "\n",
    "[token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "# Output: ['life', 'give', 'lemon', 'lemonade']\n",
    "\n",
    "\n",
    "\n",
    "# Convert back to sentence format\n",
    "\n",
    "norm = [token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "' '.join(norm) # Output: 'life give lemon lemonade'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2840f",
   "metadata": {},
   "source": [
    "### Working with Stop Words\n",
    "\n",
    "Stop words are common words that typically don't carry much meaning for text analysis (e.g., \"the\", \"is\", \"at\"). Let's explore how to identify and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc43e835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       life give lemon lemonade\n",
       "1                     buy 2 lemon 1 maven market\n",
       "2          dozen lemon gallon lemonade allrecipe\n",
       "3            lemon lemon lemon lemon lemon lemon\n",
       "4          s run market lemon s great sale today\n",
       "5    maven market carry eureka lemon meyer lemon\n",
       "6       arnold palmer half lemonade half ice tea\n",
       "7                               ice tea favorite\n",
       "Name: clean_sentence, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Function for lemmatization and stop word removal\n",
    "\n",
    "def  token_lemma_stopw(text):\n",
    "\n",
    "     doc = nlp(text)\n",
    "\n",
    "     output = [token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "     return  ' '.join(output)\n",
    "\n",
    "\n",
    "\n",
    "# Apply to entire dataset\n",
    "\n",
    "spacy_df.clean_sentence.apply(token_lemma_stopw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "658b950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def  lower_replace(series):\n",
    "\n",
    "     output = series.str.lower()\n",
    "\n",
    "     combined = r'https?://\\S+|www\\.\\S+|<.*?>|\\S+@\\S+\\.\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s]'\n",
    "\n",
    "     output = output.str.replace(combined, ' ', regex=True)\n",
    "\n",
    "     return output\n",
    "\n",
    "\n",
    "\n",
    "def  nlp_pipeline(series):\n",
    "\n",
    "     output = lower_replace(series)\n",
    "\n",
    "     output = output.apply(token_lemma_stopw)\n",
    "\n",
    "     return output\n",
    "\n",
    "\n",
    "\n",
    "# Apply complete pipeline\n",
    "\n",
    "cleaned_text = nlp_pipeline(data_df.sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Save processed data for future use\n",
    "\n",
    "pd.to_pickle(cleaned_text, 'preprocessed_text.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a1dd8",
   "metadata": {},
   "source": [
    "### Creating a Complete NLP Pipeline\n",
    "\n",
    "Now let's combine all our preprocessing steps into a single pipeline function and save the processed data for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11f8222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m92.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in ./lib/python3.11/site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m99.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m166.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1709c",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction with Scikit-Learn\n",
    "\n",
    "Now we'll convert our preprocessed text into numerical features that machine learning algorithms can work with. We'll explore two main approaches:\n",
    "\n",
    "1. **Bag of Words (Count Vectorizer)** - Counts word occurrences\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)** - Weighs words by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2018ecb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allrecipe</th>\n",
       "      <th>arnold</th>\n",
       "      <th>buy</th>\n",
       "      <th>carry</th>\n",
       "      <th>dozen</th>\n",
       "      <th>eureka</th>\n",
       "      <th>favorite</th>\n",
       "      <th>gallon</th>\n",
       "      <th>give</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>life</th>\n",
       "      <th>market</th>\n",
       "      <th>maven</th>\n",
       "      <th>meyer</th>\n",
       "      <th>palmer</th>\n",
       "      <th>run</th>\n",
       "      <th>sale</th>\n",
       "      <th>tea</th>\n",
       "      <th>today</th>\n",
       "      <th>wikipedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   allrecipe  arnold  buy  carry  dozen  eureka  favorite  gallon  give  \\\n",
       "0          0       0    0      0      0       0         0       0     1   \n",
       "1          0       0    1      0      0       0         0       0     0   \n",
       "2          1       0    0      0      1       0         0       1     0   \n",
       "3          0       0    0      0      0       0         0       0     0   \n",
       "4          0       0    0      0      0       0         0       0     0   \n",
       "5          0       0    0      1      0       1         0       0     0   \n",
       "6          0       1    0      0      0       0         0       0     0   \n",
       "7          0       0    0      0      0       0         1       0     0   \n",
       "\n",
       "   great  ...  life  market  maven  meyer  palmer  run  sale  tea  today  \\\n",
       "0      0  ...     1       0      0      0       0    0     0    0      0   \n",
       "1      0  ...     0       1      1      0       0    0     0    0      0   \n",
       "2      0  ...     0       0      0      0       0    0     0    0      0   \n",
       "3      0  ...     0       0      0      0       0    0     0    0      0   \n",
       "4      1  ...     0       1      0      0       0    1     1    0      1   \n",
       "5      0  ...     0       1      1      1       0    0     0    0      0   \n",
       "6      0  ...     0       0      0      0       1    0     0    1      0   \n",
       "7      0  ...     0       0      0      0       0    0     0    1      0   \n",
       "\n",
       "   wikipedia  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          1  \n",
       "7          0  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load preprocessed data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "series = pd.read_pickle('preprocessed_text.pkl')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Create Count Vectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "bow = cv.fit_transform(series)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "\n",
    "pd.DataFrame(bow.toarray(), columns=cv.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9904e",
   "metadata": {},
   "source": [
    "### 3.1 Bag of Words (Count Vectorizer)\n",
    "\n",
    "The Count Vectorizer creates a matrix where each row represents a document and each column represents a unique word. The values indicate how many times each word appears in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "007aed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count Vectorizer with filtering\n",
    "\n",
    "cv1 = CountVectorizer(\n",
    "\n",
    "stop_words='english', # Remove English stop words\n",
    "\n",
    "ngram_range=(1,1), # Use only single words (unigrams)\n",
    "\n",
    "min_df=2  # Include words that appear in at least 2 documents\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "bow1 = cv1.fit_transform(series)\n",
    "\n",
    "bow1_df = pd.DataFrame(bow1.toarray(), columns=cv1.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Calculate term frequencies\n",
    "\n",
    "term_freq = bow1_df.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08ace2",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Count Vectorizer Features\n",
    "\n",
    "Let's explore more sophisticated features of the Count Vectorizer:\n",
    "- **Stop word filtering**: Remove common words automatically\n",
    "- **N-gram range**: Control whether to use single words or word combinations\n",
    "- **Min document frequency**: Filter out rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c932dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Basic TF-IDF vectorization\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "\n",
    "tvidf = tv.fit_transform(series)\n",
    "\n",
    "tvidf_df = pd.DataFrame(tvidf.toarray(), columns=tv.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF with filtering\n",
    "\n",
    "tv1 = TfidfVectorizer(min_df=2) # Words must appear in at least 2 documents\n",
    "\n",
    "tvidf1 = tv1.fit_transform(series)\n",
    "\n",
    "tvidf1_df = pd.DataFrame(tvidf1.toarray(), columns=tv1.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b968e35",
   "metadata": {},
   "source": [
    "### 3.3 TF-IDF Vectorization\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a more sophisticated approach that:\n",
    "- **TF (Term Frequency)**: Measures how frequently a term appears in a document\n",
    "- **IDF (Inverse Document Frequency)**: Measures how rare or common a term is across all documents\n",
    "- **TF-IDF Score**: TF Ã— IDF - gives higher weights to terms that are frequent in a document but rare across the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb2ed58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemon                 1.583310\n",
       "lemon lemon           0.857624\n",
       "market                0.767950\n",
       "lemonade              0.743321\n",
       "ice tea               0.625522\n",
       "ice                   0.625522\n",
       "tea                   0.625522\n",
       "maven market          0.621858\n",
       "maven                 0.621858\n",
       "half                  0.505881\n",
       "tea favorite          0.493436\n",
       "favorite              0.493436\n",
       "buy lemon             0.439482\n",
       "buy                   0.439482\n",
       "lemon maven           0.439482\n",
       "life give             0.416207\n",
       "life                  0.416207\n",
       "give                  0.416207\n",
       "give lemon            0.416207\n",
       "lemon lemonade        0.416207\n",
       "lemonade allrecipe    0.358685\n",
       "lemon gallon          0.358685\n",
       "allrecipe             0.358685\n",
       "dozen                 0.358685\n",
       "gallon                0.358685\n",
       "dozen lemon           0.358685\n",
       "gallon lemonade       0.358685\n",
       "run                   0.319884\n",
       "sale today            0.319884\n",
       "market lemon          0.319884\n",
       "sale                  0.319884\n",
       "run market            0.319884\n",
       "great                 0.319884\n",
       "great sale            0.319884\n",
       "today                 0.319884\n",
       "lemon great           0.319884\n",
       "eureka lemon          0.302522\n",
       "lemon meyer           0.302522\n",
       "market carry          0.302522\n",
       "carry eureka          0.302522\n",
       "meyer                 0.302522\n",
       "meyer lemon           0.302522\n",
       "carry                 0.302522\n",
       "eureka                0.302522\n",
       "tea wikipedia         0.252941\n",
       "arnold palmer         0.252941\n",
       "half lemonade         0.252941\n",
       "palmer half           0.252941\n",
       "palmer                0.252941\n",
       "half ice              0.252941\n",
       "lemonade half         0.252941\n",
       "arnold                0.252941\n",
       "wikipedia             0.252941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Bigram TF-IDF (pairs of consecutive words)\n",
    "\n",
    "tv2 = TfidfVectorizer(ngram_range=(1,2)) # Include both unigrams and bigrams\n",
    "\n",
    "tvidf2 = tv2.fit_transform(series)\n",
    "\n",
    "tvidf2_df = pd.DataFrame(tvidf2.toarray(), columns=tv2.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Analyze feature importance\n",
    "\n",
    "tvidf2_df.sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ef3f3",
   "metadata": {},
   "source": [
    "### 3.4 N-gram Analysis and Feature Importance\n",
    "\n",
    "Let's explore bigrams (pairs of consecutive words) to capture more context and analyze feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5de8a",
   "metadata": {},
   "source": [
    "## 4. Summary and Conclusions\n",
    "\n",
    "In this notebook, we've successfully demonstrated a complete text processing pipeline:\n",
    "\n",
    "### Key Achievements:\n",
    "1. **Data Preprocessing**: Cleaned and normalized text data using regex and pandas\n",
    "2. **NLP Pipeline**: Implemented tokenization, lemmatization, and stop word removal with spaCy\n",
    "3. **Feature Extraction**: Created numerical representations using:\n",
    "   - Bag of Words (Count Vectorizer)\n",
    "   - TF-IDF Vectorization\n",
    "   - N-gram analysis (unigrams and bigrams)\n",
    "\n",
    "### Key Insights:\n",
    "- **Text cleaning** is crucial for consistent results\n",
    "- **Lemmatization** helps reduce vocabulary size while preserving meaning\n",
    "- **Stop word removal** focuses on meaningful content\n",
    "- **TF-IDF** often provides better features than simple word counts\n",
    "- **N-grams** capture context that individual words might miss\n",
    "\n",
    "### Next Steps:\n",
    "This preprocessed data is now ready for:\n",
    "- Machine learning classification tasks\n",
    "- Clustering analysis\n",
    "- Similarity comparisons\n",
    "- Topic modeling\n",
    "- Sentiment analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Libraries Used:**\n",
    "- `pandas` - Data manipulation\n",
    "- `spaCy` - Natural language processing\n",
    "- `scikit-learn` - Feature extraction and machine learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
